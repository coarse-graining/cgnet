{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse Graining Alanine Dipeptide: Force Matching via CGnet\n",
    "\n",
    "In this tutorial, we use CGnet [1] to perform a force-matching analysis of alanine dipeptide in order to produce a coarse-grained force field. The alanine dipeptide dataset is truncated and the tutorial does not represent a state of the art analysis; rather, it is designed to avoid time-consuming calculations and is an illustration of the basic pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and loading data\n",
    "\n",
    "First, we import all the necessary packages. Make sure that you've installed cgnet and all of its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from cgnet.feature import (ProteinBackboneFeature, ProteinBackboneStatistics,\n",
    "                           MoleculeDataset, compute_KLdivergence, compare_distributions)\n",
    "from cgnet.network import (CGnet, LinearLayer, HarmonicLayer, ForceLoss, ZscoreLayer,\n",
    "                           lipschitz_projection, dataset_loss, Simulation)\n",
    "\n",
    "import mdtraj as md\n",
    "from cgnet.molecule import CGMolecule\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load our example dataset. The 10,000 data points are spaced at 10 picosecond intervals for a total of 100 nanoseconds of simulation time (compare with the CGnet paper [1], in which the same system is analyzed with a 1 microsecond trajectory at 1 picosecond intervals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load('./data/ala2_coordinates.npy')\n",
    "forces = np.load('./data/ala2_forces.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 10,000 data points has the three-dimensional coordinates and forces of each of the five coarse-grained beads (the five backbone atoms). We verify that the dimensions of the coordinates and forces are (number of frames, number of beads, number of dimensions) by printing their shapes.\n",
    "\n",
    "Given correctly-shaped coordinates and cooresponding forces, we can make a `MoleculeDataset` that will interface with our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coordinates size: {}\".format(coords.shape))\n",
    "print(\"Force: {}\".format(forces.shape))\n",
    "\n",
    "ala_data = MoleculeDataset(coords,forces)\n",
    "print(\"Dataset length: {}\".format(len(ala_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering statistics\n",
    "\n",
    "The harmonic contributions to the free energy from bonds and angles provide an important regularizing prior form on the energy learned by CGnet. Thus, we need to gather some statistics about our data's features. Specifically, we need the `get_bond_constants` method in the `ProteinBackboneStatistics` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ProteinBackboneStatistics(coords)\n",
    "stats_dict = stats.get_bond_constants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the dictionary, we see that the means, standard deviations, and harmonic constants of the two-, three-, and four-body bonds (adjacent distances), angles, and dihedrals, respectively, are stored according to a tuple of the indices of the involves atoms. Dihedrals don't have harmonic constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Key {}: {}\".format((0,1), stats_dict[(0,1)]))\n",
    "print(\"Key {}: {}\".format((2,3,4), stats_dict[(2,3,4)]))\n",
    "print(\"Key {}: {}\".format((0,1,2,3,\"sin\"), stats_dict[(0,1,2,3,\"sin\")]))\n",
    "print(\"There are {} total features.\".format(len(stats_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make separate dictionaries for bonds and angles, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonds = {k : stats_dict[k]\n",
    "         for k in [(i, i+1) for i in range(coords.shape[1]-1)]}\n",
    "angles = {k : stats_dict[k]\n",
    "          for k in [(i, i+1, i+2) for i in range(coords.shape[1]-2)]}\n",
    "print(\"We have {} backbone beads, {} bonds, and {} angles.\".format(\n",
    "                        coords.shape[1], len(bonds), len(angles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the harmonic constants for the bonds and angles, we want the zscores for every feature, including the dihedrals and the non-bond (i.e., non-adjacent) pairwise distances. The `get_zscores` method produces a row of the means and a row of the standard deviations for every feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscores = stats.get_zscores(as_dict=False).float()\n",
    "print(\"We have {} statistics for {} features.\".format(zscores.shape[0], zscores.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to the following calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['Distances', 'Angles', 'Dihedral_cosines', 'Dihedral_sines']\n",
    "means = []\n",
    "stdevs = []\n",
    "for category in order:\n",
    "    features = stats.descriptions[category]\n",
    "    for feature in features:\n",
    "        if category == 'Dihedral_cosines':\n",
    "            feature = tuple(list(feature) + ['cos'])\n",
    "        elif category == 'Dihedral_sines':\n",
    "            feature = tuple(list(feature) + ['sin'])\n",
    "        means.append(stats_dict[feature]['mean'])\n",
    "        stdevs.append(stats_dict[feature]['std'])\n",
    "zscores_check = np.vstack([means, stdevs])\n",
    "np.testing.assert_array_equal(zscores, zscores_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our network architecture\n",
    "\n",
    "Now we are ready to start designing the architecture of our CGnet. We follow the optimal architecture in the CGnet paper [1], which consists of:\n",
    "\n",
    "- a feature layer that outputs pairwise distances, angles, and dihedral angles from the Cartesian input trajectory that subtracts the means and divides by the standard deviations,\n",
    "- 5 hidden linear layers (bias term inclusive) with 160 nodes each, with the first three followed by `Tanh()` activation, and\n",
    "- harmonic prior layers that compute prior energies from bonds and angles.\n",
    "\n",
    "The layers are stored in the list `layers` and the priors are stored in the list `priors`.\n",
    "\n",
    "<img src=\"./CGnet.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by scaling according to mean and standard deviation\n",
    "layers = [ZscoreLayer(zscores)]\n",
    "\n",
    "# The first hidden layer goes from number of features to 160\n",
    "num_feat = len(stats_dict)\n",
    "layers += LinearLayer(num_feat, 160, activation=nn.Tanh())\n",
    "\n",
    "# The inner hidden layers stay the same size\n",
    "for _ in range(5):\n",
    "    layers += LinearLayer(160, 160, activation=nn.Tanh())\n",
    "\n",
    "# The last layer produces a single value\n",
    "layers += LinearLayer(160, 1, activation=None)\n",
    "\n",
    "# Construct prior energy layers\n",
    "priors  = [HarmonicLayer(bonds,stats.descriptions,\"Distances\")]\n",
    "priors += [HarmonicLayer(angles,stats.descriptions, \"Angles\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our architecture, we are ready to build our CGnet. Because we are recovering the coarse grain potential by matching the potential of mean force (PMF) from the all-atom + solvent model of alanine dipeptide, we imbue our network with the `ForceLoss()` criterion. We can look at our network by printing the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ala2_net = CGnet(layers, ForceLoss(),\n",
    "                 feature=ProteinBackboneFeature(),\n",
    "                 priors=priors)\n",
    "print(ala2_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training routine and hyperparameters\n",
    "\n",
    "Next, we setup the training routine. Following Jiang et al (2019), we sample randomly from the dataset with a batch size of 512, Adam optimization with an intial learning rate of 0.003, epochal multiplicative learning rate decay with decay constant $\\gamma=0.3$, and L2 Lipschitz regularization via spectral normalization ($\\lambda=4.0$) of each weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "batch_size = 512\n",
    "learning_rate = 0.003\n",
    "rate_decay = 0.3\n",
    "lipschitz_strength = 4.0\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "save_model = True\n",
    "directory = '.' # to save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our training tools. We will neglect cross-validation for brevity, but this is __not__ acceptable for real analysis. To do cross-validation, a `testloader` would be needed with validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training tools\n",
    "\n",
    "trainloader = DataLoader(ala_data, sampler=RandomSampler(ala_data),\n",
    "                         batch_size=batch_size)\n",
    "optimizer = torch.optim.Adam(ala2_net.parameters(),\n",
    "                             lr=learning_rate)\n",
    "scheduler = MultiStepLR(optimizer,milestones=[1,2,3,4,5],\n",
    "                        gamma=rate_decay)\n",
    "epochal_train_losses = []\n",
    "epochal_test_losses  = []\n",
    "verbose = True\n",
    "batch_freq = 500\n",
    "epoch_freq = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "\n",
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    scheduler.step()\n",
    "    test_loss = 0.00\n",
    "    for num, batch in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        coord, force = batch\n",
    "        energy, pred_force = ala2_net.forward(coord)\n",
    "        batch_loss = ala2_net.criterion(pred_force, force)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # perform L2 lipschitz check and projection\n",
    "        lipschitz_projection(ala2_net, strength=lipschitz_strength)\n",
    "        if verbose:\n",
    "            if (num+1) % batch_freq == 0:\n",
    "                print(\n",
    "                    \"Batch: {}\\tTrain: {}\\tTest: {}\".format(\n",
    "                        num+1, batch_loss, test_loss)\n",
    "                )\n",
    "    train_loss = dataset_loss(ala2_net, trainloader)\n",
    "    #test_loss = dataset_loss(ala2_net, self.testloader).data\n",
    "    if verbose:\n",
    "        if epoch % epoch_freq == 0:\n",
    "            print(\n",
    "                \"Epoch: {}\\tTrain: {}\\tTest: {}\".format(\n",
    "    epoch, train_loss, test_loss))\n",
    "        epochal_train_losses.append(train_loss)\n",
    "        #epochal_test_losses.append(test_loss)\n",
    "        \n",
    "if save_model:\n",
    "    torch.save(ala2_net,\"{}/ala2_net.pt\".format(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot at the training loss over the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(0,len(epochal_train_losses),1),\n",
    "         epochal_train_losses, label='Training Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(np.arange(1,5))\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the coarse-grained force field to simulate system dynamics\n",
    "\n",
    "First we set up initial coordinates which will be used as the starting points for independent simulations.\n",
    "\n",
    "We will run 1000 independent simulations seeded evenly the original dataset for 1000 timesteps each. These paramters can be changed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 100\n",
    "n_timesteps = 10000\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_coords = np.concatenate([coords[i].reshape(-1,5,3)\n",
    "                                 for i in np.arange(0, 10000, 10000//n_sims)],\n",
    "                                 axis=0)\n",
    "initial_coords = torch.tensor(initial_coords, requires_grad=True)\n",
    "print(\"Produced {} initial coordinates.\".format(len(initial_coords)))\n",
    "\n",
    "sim = Simulation(ala2_net, initial_coords, length=n_timesteps,\n",
    "                 save_interval=save_interval, beta=stats.beta,\n",
    "                 verbose=True, save_potential=True)\n",
    "\n",
    "traj = sim.simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the simulation output\n",
    "\n",
    "Now we want to see what our simulated coarse-grained dynamics are like. First we create an `mdtraj`-friendly [2] `CGMolecule` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['C', 'N', 'CA', 'C', 'N']\n",
    "resseq = [1, 2, 2, 2, 3]\n",
    "resmap = {1: 'ACE', 2: 'ALA', 3: 'NME'}\n",
    "\n",
    "ala2_cg = CGMolecule(names=names, resseq=resseq, resmap=resmap,\n",
    "                          bonds='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can input our training and simulated coordinates in order to make `mdtraj` trajectories out of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ala2_traj = ala2_cg.make_trajectory(coords)\n",
    "ala2_simulated_traj = ala2_cg.make_trajectory(np.concatenate(traj, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `mdtraj`, we compute the backbone $\\phi$ and $\\psi$ torsional angles of our reference and simulated systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, phi = md.compute_phi(ala2_traj)\n",
    "_, psi = md.compute_psi(ala2_traj)\n",
    "\n",
    "_, sim_phi = md.compute_phi(ala2_simulated_traj)\n",
    "_, sim_psi = md.compute_psi(ala2_simulated_traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reference coordinates, we compute the potential using our network. For the simulated coordinates, we saved the potential when we ran the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot, _ = ala2_net.forward(torch.tensor(coords, requires_grad=True))\n",
    "pot = pot.detach().numpy()\n",
    "pot = pot - np.min(pot)\n",
    "\n",
    "sim_pot = np.concatenate(sim.simulated_potential, axis=0)\n",
    "sim_pot = sim_pot - np.min(sim_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the reference and simulation Ramachandran plots. Since we didn't train for very long, and because we used only a small amount of data, this shouldn't be too impressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(phi.reshape(-1), psi.reshape(-1),\n",
    "            c=pot.flatten(),\n",
    "            cmap=plt.get_cmap(\"viridis\"),alpha=0.5,s=0.5)\n",
    "plt.xlabel(r'$\\phi$',fontsize=16)\n",
    "plt.ylabel(r'$\\psi$',fontsize=16)\n",
    "plt.xlim(-np.pi, np.pi)\n",
    "plt.ylim(-np.pi, np.pi)\n",
    "plt.title(r'Original all-atom trajectory')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title(r'$U\\left(\\frac{kcal}{mol}\\right)$')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(sim_phi.reshape(-1), sim_psi.reshape(-1),\n",
    "            c=sim_pot.flatten(),\n",
    "            cmap=plt.get_cmap(\"viridis\"),alpha=0.5,s=0.5)\n",
    "plt.xlabel(r'$\\phi$',fontsize=16)\n",
    "plt.ylabel(r'$\\psi$',fontsize=16)\n",
    "plt.xlim(-np.pi, np.pi)\n",
    "plt.ylim(-np.pi, np.pi)\n",
    "plt.title('Simulated CG trajectory')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title(r'$U\\left(\\frac{kcal}{mol}\\right)$')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the simulated free energy $F = U - TS$. To do this, we histogram the 2-dimensional dihedral trajectory and convert counts to populations $\\pi_i$ for each bin $i$. Then, we transform the nonzero populations into energies according to $F_i = \\beta^{-1}\\log\\pi_i$.\n",
    "\n",
    "First we set up a graphing utility for our Ramachandran plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ramachandran(phi, psi, bins=60, cmap=plt.cm.magma):\n",
    "    edges = np.array([[-np.pi, np.pi], [-np.pi, np.pi]])\n",
    "    counts, _, _ = np.histogram2d(psi.reshape(-1),\n",
    "                                  phi.reshape(-1),\n",
    "                                  bins=bins,\n",
    "                                  range=edges)\n",
    "    populations = counts / np.sum(counts)\n",
    "    \n",
    "    # compute energies for only non-zero entries\n",
    "    # 1/beta is approximately 0.6 kcal/mol at 300 K\n",
    "    energies = -0.6*np.log(populations,\n",
    "                           out=np.zeros_like(populations),\n",
    "                           where=(populations > 0))\n",
    "    \n",
    "    # make the lowest energy slightly above zero\n",
    "    energies = np.where(energies,\n",
    "                        energies-np.min(energies[np.nonzero(energies)]) + 1e-6,\n",
    "                        0)\n",
    "    \n",
    "    # mask the zero values from the colormap\n",
    "    zvals_masked = np.ma.masked_where(energies == 0, energies)\n",
    "\n",
    "    cmap.set_bad(color='white')\n",
    "    img = plt.imshow(zvals_masked, interpolation='nearest', cmap = cmap)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.xticks([-0.5, bins/2, bins], \n",
    "               [r'$-\\pi$', r'$0$', r'$\\pi$'])\n",
    "\n",
    "    plt.yticks([-0.5, bins/2, bins],\n",
    "               [r'$-\\pi$', r'$0$', r'$\\pi$'])\n",
    "    \n",
    "    plt.xlabel(r'$\\phi$',fontsize=16)\n",
    "    plt.ylabel(r'$\\psi$',fontsize=16)\n",
    "    \n",
    "    cb=plt.colorbar()\n",
    "    cb.ax.set_title(r'$\\tilde{F}\\left(\\frac{kcal}{mol}\\right)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the free energies for the original and CG trajectories. Again, we expect this to look bad because we used only 1% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plot_ramachandran(phi, psi)\n",
    "plt.title('Original all-atom trajectory')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_ramachandran(sim_phi, sim_psi)\n",
    "plt.title('Simulated CG trajectory')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. We can also inspect how the distributions of features from the CG simulation compare to that of the original simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_idx = [[i,i+1] for i in range(len(bonds))]\n",
    "ang_idx = [[i,i+1,i+2] for i in range(len(angles))]\n",
    "\n",
    "bond_traj = md.compute_distances(ala2_traj, bond_idx)\n",
    "ang_traj = md.compute_angles(ala2_traj, ang_idx)\n",
    "sim_bond_traj = md.compute_distances(ala2_simulated_traj, bond_idx)\n",
    "sim_ang_traj = md.compute_angles(ala2_simulated_traj, ang_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs=[]\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "for i in range(bond_traj.shape[1]):\n",
    "    plt.subplot(round(len(bonds)/2),2, i+1)\n",
    "    hist1, hist2, bins, div = compare_distributions(bond_traj[:,i],sim_bond_traj[:,i],\n",
    "                                                    nbins=60, compute_overlap='kl_div')\n",
    "    \n",
    "    divs.append(div)\n",
    "    plt.hist(bond_traj[:,i], bins=bins, alpha=0.5,\n",
    "             label='Reference bond {}'.format(i+1),density=True)\n",
    "    plt.hist(sim_bond_traj[:,i], bins=bins, alpha=0.5,\n",
    "             label='CGsim bond {}'.format(i+1),density=True)\n",
    "    plt.title = \"Bond {}\".format(i+1)\n",
    "    plt.xlabel('Angstroms')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='best')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a measure distributional overlap, we compute the discrete Kullback-Leibler (KL) divergence as follows:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    D_{KL} (\\mathcal{B}_{CG} || \\mathcal{B}_{ref}) = \\sum_{bin}^{nbins} \\mathcal{B}_{CG}(bin) \\log{\\frac{\\mathcal{B}_{CG}(bin)}{\\mathcal{B}_{ref}(bin)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(divs)+1,1), divs)\n",
    "plt.xlabel('Bond Index')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, in accordance with plots above, the CG simulation can reproduce all of the bond distributions well, especially the two internal bonds. We can perform the same analyses for angles, dihedrals, and non-bonded pairs as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs=[]\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "for i in range(ang_traj.shape[1]):\n",
    "    plt.subplot(round(len(angles)/2),2, i+1)\n",
    "    hist1, hist2, bins, div = compare_distributions(ang_traj[:,i],sim_ang_traj[:,i],\n",
    "                                                    nbins=60, compute_overlap='kl_div')\n",
    "    \n",
    "    divs.append(div)\n",
    "    plt.hist(ang_traj[:,i], bins=bins, alpha=0.5,\n",
    "             label='Reference angle {}'.format(i+1),density=True)\n",
    "    plt.hist(sim_ang_traj[:,i], bins=bins, alpha=0.5,\n",
    "             label='CGsim angle {}'.format(i+1),density=True)\n",
    "    plt.title = \"Angle {}\".format(i+1)\n",
    "    plt.xlabel('Radians')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='best')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(divs)+1,1), divs)\n",
    "plt.xlabel('Angle Index')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dihedrals = np.hstack((phi,psi))\n",
    "sim_dihedrals = np.hstack((sim_phi,sim_psi))\n",
    "divs=[]\n",
    "fig = plt.figure(figsize=(16,3))\n",
    "for i in range(dihedrals.shape[1]):\n",
    "    plt.subplot(1,2, i+1)\n",
    "    hist1, hist2, bins, div = compare_distributions(dihedrals[:,i],sim_dihedrals[:,i],\n",
    "                                                    nbins=60, compute_overlap='kl_div')\n",
    "    \n",
    "    divs.append(div)\n",
    "    plt.hist(dihedrals[:,i], bins=bins, alpha=0.5,\n",
    "             label='Reference dihedral {}'.format(i+1),density=True)\n",
    "    plt.hist(sim_dihedrals[:,i], bins=bins, alpha=0.5,\n",
    "             label='CGsim dihedral {}'.format(i+1),density=True)\n",
    "    plt.title = \"Dihedral {}\".format(i+1)\n",
    "    plt.xlabel('Radians')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='best')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(divs)+1,1), divs)\n",
    "plt.xlabel('Dihedral Index')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_idx = [[0,2],[0,3],[0,4],[1,3],[1,4],[2,4]]\n",
    "dist_traj = md.compute_distances(ala2_traj, dist_idx)\n",
    "sim_dist_traj = md.compute_distances(ala2_simulated_traj, dist_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divs=[]\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "for i in range(dist_traj.shape[1]):\n",
    "    plt.subplot(round(len(dist_idx)/2),2, i+1)\n",
    "    hist1, hist2, bins, div = compare_distributions(dist_traj[:,i],sim_dist_traj[:,i],\n",
    "                                                    nbins=60, compute_overlap='kl_div')\n",
    "    \n",
    "    divs.append(div)\n",
    "    plt.hist(dist_traj[:,i], bins=bins, alpha=0.5,\n",
    "             label='Reference distance {}'.format(i+1),density=True)\n",
    "    plt.hist(sim_dist_traj[:,i], bins=bins, alpha=0.5,\n",
    "             label='CGsim distance {}'.format(i+1),density=True)\n",
    "    plt.title = \"Distance {}\".format(i+1)\n",
    "    plt.xlabel('Angstroms')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='best')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(divs)+1,1), divs)\n",
    "plt.xlabel('Distance Pair Index')\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *References*\n",
    "\n",
    "[1] Wang, J., Olsson, S., Wehmeyer, C., Pérez, A., Charron, N. E., de Fabritiis, G., Noé, F., and Clementi, C. (2019). Machine Learning of Coarse-Grained Molecular Dynamics Force Fields. _ACS Central Science._ https://doi.org/10.1021/acscentsci.8b00913\n",
    "\n",
    "[2] McGibbon, R. T., Beauchamp, K. A., Harrigan, M. P., Klein, C., Swails, J. M., Hernández, C. X., Schwantes, C. R., Wang, L.-P., Lane, T. J., and Pande, V. S. (2015). MDTraj: A Modern Open Library for the Analysis of Molecular Dynamics Trajectories. _Biophys J._ http://dx.doi.org/10.1016/j.bpj.2015.08.015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
