{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Sized Data With CGSchNet\n",
    "\n",
    "In developing transferable CG protein force fields, it is often advantageous to use data from multiple molecules. This can naturally lead to datasets composed of differently sized molecules. This notebook walks through an example of how to work with such datasets using the CGSchNet framework. Please note that the dataset that we use here is only for demonstration purposes, and the results within this notebook cannot be used to draw any conclusions. Furthermore, the functionalites demonstrated below are only compatible with SchNet-related tools within CGnet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and loading data\n",
    "First,  we import all necessary tools for this notebook. Then we will load a reduced dialanine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from cgnet.feature import (SchnetFeature, GeometryStatistics,\n",
    "                           MultiMoleculeDataset, LinearLayer,\n",
    "                           multi_molecule_collate, CGBeadEmbedding)\n",
    "from cgnet.network import (CGnet, HarmonicLayer, ForceLoss, ZscoreLayer,\n",
    "                           lipschitz_projection, dataset_loss, Simulation)\n",
    "\n",
    "import mdtraj as md\n",
    "from cgnet.molecule import CGMolecule\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# We specify the CPU as the training/simulating device here.\n",
    "# If you have machine  with a GPU, you can use the GPU for\n",
    "# accelerated training/simulation by specifying \n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "# Here, we are seeding the numpy random number generator so that the same pseudomolecule\n",
    "# sizes are used in the dataset each time the notebook is run. \n",
    "np.random.seed(1133)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dialnine dataset, which has been reduced in size for the purpose of easy demonstration. We also create atomic embeddings as in the CG-Force-Fields-With-SchNet-Embeddings notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load('./data/ala2_coordinates.npy')\n",
    "forces = np.load('./data/ala2_forces.npy')\n",
    "embeddings = np.tile([6, 7, 6, 6, 7], [coords.shape[0], 1])\n",
    "\n",
    "print(\"Coordinates size: {}\".format(coords.shape))\n",
    "print(\"Forces size: {}\".format(forces.shape))\n",
    "print(\"Embeddings size: {}\".format(embeddings.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a dataset for variable length molecules\n",
    "\n",
    "In order to simulate a dataset that contains proteins of various sizes, we take each example in the dataset and randomly truncate the number of beads in the coordinates and the forces. These \"varied\" examples can range from 2 to the full 5 CG beads of dialanine. As such, the indivudual exmaples are entirely artificial and meaningless and are used for demonstration only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_lengths = np.random.randint(2, high=6, size=len(coords))\n",
    "\n",
    "varied_coords = []\n",
    "varied_forces = []\n",
    "varied_embeddings = []\n",
    "\n",
    "for num, length in enumerate(random_lengths):\n",
    "    varied_coords.append(coords[num, :length, :])\n",
    "    varied_forces.append(forces[num, :length, :])\n",
    "    varied_embeddings.append(embeddings[num, :length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variable length functionality is not (yet) compatible with using `GeometryStatistics`, which would normally be used to help set up priors for fixed-length molecules. Instead, we seek to demonstrate a usage of CGSchNet in the same spirit of the original SchNet architecture [2] which does not use priors. For simplicity here, we assume that priors are not needed (which is in general not a safe assumption for learning CG force fields, as noted by [1]). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no need to instance a `GeometryStatistics` object, we move onto constructing our dataset. This is done using the `MultiMoleculeDataset` object. This object is used to handle and assemble datasets that contain multiple molecules of variable lengths. It is intialized by passing __*lists of variable length coordinates, forces, and embeddings*__, where these three lists are all in the same order (e.g., `varied_coords[2]`, `varied_list[2]`, and `varied_embeddings[2]` give the coordinates, forces, and embeddings of the example at index 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varied_dataset = MultiMoleculeDataset(varied_coords, varied_forces, varied_embeddings)\n",
    "print(\"Dataset length: {}\".format(len(varied_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the first 3 examples from this varied molecule dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 1, 2]\n",
    "for index in indices:\n",
    "    output = varied_dataset.__getitem__([index])[0]\n",
    "    print(\"Coordinates size: {}\".format(output['coords'].shape))\n",
    "    print(output['coords'])\n",
    "    print(\"Forces size: {}\".format(output['forces'].shape))\n",
    "    print(output['forces'])\n",
    "    print(\"Embeddings size: {}\".format(output['embeddings'].shape))\n",
    "    print(output['embeddings'], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these three examples contain proteins of lenghts 2, 5, and 4 beads respectively (for numpy random seed 1133). We can also see that `MultiMoleculeDataset` returns a dictionary for each example, with the keys `coords`, `forces`, and `embeddings` with the corresponding coordinates, forces, and embeddings for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding our input into the model\n",
    "\n",
    "With out dataset set up, we have to ask \"how can a neural network handle variable length data\". The answer, which is inspired by both [SchNetPack](https://github.com/atomistic-machine-learning/schnetpack) [3] and typical practices taken from natural language processing is __*input padding and masking*__. Input padding is preprocessing method that ensures all examples fed into a neural network in a batch are of the same size. This is accomplished by inserting or \"padding\" smaller size examples in the batch with 0's such that all examples in the batch are the same size as the largest example in the batch. Luckily, PyTorch has recurrent neural network utilities that add these and other related padding functionalities. We have used these to create a special __*collating function*__ called `multi_molecule_collate` to work with `MultiMoleculeDataset`. This collating function must be passed to the `collate_fn` kwarg of a PyTorch `DataLoader` object, which combines a dataset with a sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(varied_dataset, batch_size=3, shuffle=False, collate_fn=multi_molecule_collate)\n",
    "\n",
    "for num, batch in enumerate(loader):\n",
    "    coords, forces, embeddings = batch\n",
    "    print(\"Coordinates size:\", coords.size())\n",
    "    print(coords)\n",
    "    print(\"Forces size:\", forces.size())\n",
    "    print(forces)\n",
    "    print(\"Embeddings size:\", embeddings.size())\n",
    "    print(embeddings)\n",
    "    break # we just want to grab only the first batch from the data loader, so we break here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that padding gives us a way to stack all of the coordinates, forces, and embeddings into respective torch tensors that can be passed to a model. Note the locations of the padding zeros in relation to the raw ouput of `MultiMoleculeDataset` above. We can see that padding zeros have been appended to the ends of the smaller molecules' embeddings to reach the size of the embeddings of the largest molecule in the batch. Similarly, padding zeros have also been appended to the the coordinates and forces of the smaller molecules in the batch, such that their number of beads is artifically extended to match the number of beads of the largest molecule in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "Now that we have a way to properly pad input from a dataset of variable length molecules, we can create our model and begin training. Here, we make a \"classic\" CGSchNet model in which the `CGnet` feature is just a `SchnetFeature` (no `GeometryFeature` or `FeatureCombiner` here) with the following architectural hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "n_layers = 5\n",
    "n_nodes = 128\n",
    "activation = nn.Tanh()\n",
    "batch_size = 512\n",
    "learning_rate = 3e-4\n",
    "rate_decay = 0.3\n",
    "lipschitz_strength = 4.0\n",
    "\n",
    "# schnet-specific parameters\n",
    "n_embeddings = 10\n",
    "n_gaussians = 50\n",
    "n_interaction_blocks = 5\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "save_model = False\n",
    "directory = '.' # to save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure to set the embeddings to the maximum number for the atomic embeddings of this system. Remember, by default we cannot use 0's for embedding integers - these are reserved for padding integers as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = CGBeadEmbedding(n_embeddings, n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becasue we are not using a `FeatureCombiner`/`GeometryFeature`, we must specify `calculate_geometry=True` in the `SchnetFeature` initialization. This allows the `SchnetFeature` to calculate pairwise distances on the fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schnet_feature = SchnetFeature(feature_size=n_nodes,\n",
    "                               embedding_layer=embedding_layer,\n",
    "                               n_interaction_blocks=n_interaction_blocks,\n",
    "                               calculate_geometry=True,\n",
    "                               activation=activation,\n",
    "                               n_beads=10,\n",
    "                               neighbor_cutoff=None,\n",
    "                               n_gaussians=n_gaussians,\n",
    "                               device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we assemble the `CGnet` model as usual. As a reminder, we are not using prior energy terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layers = LinearLayer(n_nodes,\n",
    "                     n_nodes,\n",
    "                     activation=activation)\n",
    "\n",
    "for _ in range(n_layers - 1):\n",
    "    layers += LinearLayer(n_nodes,\n",
    "                          n_nodes,\n",
    "                          activation=activation)\n",
    "\n",
    "# The last layer produces a single value\n",
    "layers += LinearLayer(n_nodes, 1, activation=None)\n",
    "\n",
    "variable_model = CGnet(layers, ForceLoss(),\n",
    "                 feature=schnet_feature,\n",
    "                 priors=None).to(device)\n",
    "print(variable_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model\n",
    "\n",
    "With our model and dataset all set, we create a new dataloader for training (the old one used above was just for demonstration purposes). We also create an optimizer and a learning rate scheduler for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(varied_dataset, batch_size=512, shuffle=True, collate_fn=multi_molecule_collate)\n",
    "optimizer = torch.optim.Adam(variable_model.parameters(),\n",
    "                             lr=learning_rate)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[10, 20, 30, 40, 50],\n",
    "                        gamma=rate_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we begin training. Here, we are not using a test set. __*This is done only for ease of demonstration. In production/research endeavors, a dedicated test set or cross validation strategy is essential for accurate model assessment*__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are surpressing batchwise printouts, by setting batch_freq=np.inf.\n",
    "# As a consequence, printouts during training will only occur onece every epoch\n",
    "\n",
    "batch_freq = np.inf \n",
    "verbose = True\n",
    "epoch_freq = 1\n",
    "epochal_train_losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train_loss = dataset_loss(variable_model, loader,\n",
    "                              optimizer,\n",
    "                              verbose_interval=batch_freq)\n",
    "\n",
    "    scheduler.step()\n",
    "    epochal_train_losses.append(train_loss)\n",
    "    \n",
    "if save_model:\n",
    "    torch.save(variable_model, \"{}/variable_model.pt\".format(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(0, len(epochal_train_losses), 1),\n",
    "         epochal_train_losses, label='Training Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "With our model trained, we can perform simulation just as we normally would. Here we will simulate a molecule with 5 beads (the largest size molecule our model can handle, and the largest possible molecule in our dataset). Keep in mind that because our dataset is artificial, no meaningful conclusions can be drawn from the simulation results. It is only meant for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sims = 100\n",
    "n_timesteps = 1000\n",
    "save_interval = 10\n",
    "\n",
    "KBOLTZMANN = 1.38064852e-23\n",
    "AVOGADRO = 6.022140857e23\n",
    "JPERKCAL = 4184\n",
    "temperature = 300\n",
    "beta = JPERKCAL / KBOLTZMANN / AVOGADRO / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load('./data/ala2_coordinates.npy')\n",
    "forces = np.load('./data/ala2_forces.npy')\n",
    "embeddings = np.tile([6, 7, 6, 6, 7], [coords.shape[0], 1])\n",
    "\n",
    "initial_coords = np.concatenate([coords[i].reshape(-1, 5, 3)\n",
    "                                 for i in np.arange(0, coords.shape[0],\n",
    "                                                    coords.shape[0]//n_sims)],\n",
    "                                                    axis=0)\n",
    "initial_coords = torch.tensor(initial_coords, requires_grad=True).to(device)\n",
    "sim_embeddings = torch.tensor(embeddings[:n_sims]).to(device)\n",
    "\n",
    "print(\"Produced {} initial coordinates.\".format(len(initial_coords)))\n",
    "variable_model.eval()\n",
    "sim = Simulation(variable_model, initial_coords, sim_embeddings, length=n_timesteps,\n",
    "                 save_interval=save_interval, beta=beta,\n",
    "                 save_potential=True, device=device,\n",
    "                 log_interval=100, log_type='print')\n",
    "\n",
    "traj = sim.simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['C', 'N', 'CA','C', 'N']\n",
    "resseq = [1, 2, 2, 2, 3]\n",
    "\n",
    "resmap = {1: 'ACE', 2: 'ALA', 3: 'NME'}\n",
    "\n",
    "ala2_cg = CGMolecule(names=names, resseq=resseq, resmap=resmap,\n",
    "                          bonds='standard')\n",
    "\n",
    "ala2_traj = ala2_cg.make_trajectory(coords)\n",
    "ala2_simulated_traj = ala2_cg.make_trajectory(np.concatenate(traj, axis=0))\n",
    "\n",
    "beads = [(0, 1, 2, 3), (1, 2, 3, 4)]\n",
    "\n",
    "dihedrals_ref = md.compute_dihedrals(ala2_traj, np.vstack(beads))\n",
    "dihedrals_cg = md.compute_dihedrals(ala2_simulated_traj, np.vstack(beads))\n",
    "\n",
    "pot, _ = variable_model.forward(torch.tensor(coords, requires_grad=True),\n",
    "                          torch.tensor(embeddings))\n",
    "pot = pot.detach().numpy()\n",
    "pot = pot - np.min(pot)\n",
    "\n",
    "sim_pot = np.concatenate(sim.simulated_potential, axis=0)\n",
    "sim_pot = sim_pot - np.min(sim_pot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(dihedrals_ref[:, 0].reshape(-1), dihedrals_ref[:, 1].reshape(-1),\n",
    "            c=pot.flatten(), cmap=plt.get_cmap(\"viridis\"), alpha=0.5, s=0.5)\n",
    "plt.xlabel(r'$\\phi$', fontsize=16)\n",
    "plt.ylabel(r'$\\psi$', fontsize=16)\n",
    "plt.xlim(-np.pi, np.pi)\n",
    "plt.ylim(-np.pi, np.pi)\n",
    "plt.title(r'Original all-atom trajectory')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title(r'$U\\left(\\frac{kcal}{mol}\\right)$')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(dihedrals_cg[:, 0].reshape(-1), dihedrals_cg[:, 1].reshape(-1),\n",
    "            c=sim_pot.flatten(), cmap=plt.get_cmap(\"viridis\"), alpha=0.5, s=0.5)\n",
    "plt.xlabel(r'$\\phi$', fontsize=16)\n",
    "plt.ylabel(r'$\\psi$', fontsize=16)\n",
    "plt.xlim(-np.pi, np.pi)\n",
    "plt.ylim(-np.pi, np.pi)\n",
    "plt.title('Simulated CG trajectory')\n",
    "clb=plt.colorbar()\n",
    "clb.ax.set_title(r'$U\\left(\\frac{kcal}{mol}\\right)$')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ramachandran(phi, psi, bins=60, cmap=plt.cm.magma):\n",
    "    edges = np.array([[-np.pi, np.pi], [-np.pi, np.pi]])\n",
    "    counts, _, _ = np.histogram2d(psi.reshape(-1),\n",
    "                                  phi.reshape(-1),\n",
    "                                  bins=bins,\n",
    "                                  range=edges)\n",
    "    populations = counts / np.sum(counts)\n",
    "    \n",
    "    # compute energies for only non-zero entries\n",
    "    # 1/beta is approximately 0.6 kcal/mol at 300 K\n",
    "    energies = -0.6*np.log(populations,\n",
    "                           out=np.zeros_like(populations),\n",
    "                           where=(populations > 0))\n",
    "    \n",
    "    # make the lowest energy slightly above zero\n",
    "    energies = np.where(energies,\n",
    "                        energies-np.min(energies[np.nonzero(energies)]) + 1e-6,\n",
    "                        0)\n",
    "    \n",
    "    # mask the zero values from the colormap\n",
    "    zvals_masked = np.ma.masked_where(energies == 0, energies)\n",
    "\n",
    "    cmap.set_bad(color='white')\n",
    "    img = plt.imshow(zvals_masked, interpolation='nearest', cmap=cmap)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.xticks([-0.5, bins / 2, bins], \n",
    "               [r'$-\\pi$', r'$0$', r'$\\pi$'])\n",
    "\n",
    "    plt.yticks([-0.5, bins / 2, bins],\n",
    "               [r'$-\\pi$', r'$0$', r'$\\pi$'])\n",
    "    \n",
    "    plt.xlabel(r'$\\phi$', fontsize=16)\n",
    "    plt.ylabel(r'$\\psi$', fontsize=16)\n",
    "    \n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.set_title(r'$\\tilde{F}\\left(\\frac{kcal}{mol}\\right)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_ramachandran(dihedrals_ref[:, 0], dihedrals_ref[:, 1])\n",
    "plt.title('Original all-atom trajectory')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_ramachandran(dihedrals_cg[:, 0], dihedrals_cg[:, 1])\n",
    "plt.title('Simulated CG trajectory')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *References*\n",
    "\n",
    "[1] Wang, J., Olsson, S., Wehmeyer, C., Pérez, A., Charron, N. E., de Fabritiis, G., Noé, F., and Clementi, C. (2019). Machine Learning of Coarse-Grained Molecular Dynamics Force Fields. _ACS Central Science._ https://doi.org/10.1021/acscentsci.8b00913\n",
    "\n",
    "[2] Schütt, K. T., Sauceda, H. E., Kindermans, P.-J., Tkatchenko, A., & Müller, K.-R. (2018). SchNet – A deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24), 241722. https://doi.org/10.1063/1.5019779\n",
    "\n",
    "[3] Schütt, K. T., Kessel, P., Gastegger, M., Nicoli, K. A., Tkatchenko, A., & Müller, K.-R. (2019). SchNetPack: A Deep Learning Toolbox For Atomistic Systems. Journal of Chemical Theory and Computation, 15(1), 448–455. https://doi.org/10.1021/acs.jctc.8b00908\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
